{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"5\">Data Science Academy</font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"4\">Deep Learning Book</font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"4\">Capítulo 69 - Agente Baseado em IA com Linguagem Python</font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Jupyter Notebook é parte integrante do Deep Learning Book:\n",
    "    \n",
    "www.deeplearningbook.com.br   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta aula considera que você possui conhecimento em Linguagem Python. Se esse não for o caso, acesse nosso curso gratuito e comece aprender agora mesmo: <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\">Python Fundamentos Para Análise de Dados</a>.\n",
    "\n",
    "Consideramos ainda que você já estudou os capítulos anteriores do Deep Learning Book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos desenvolver o algoritmo Q-learning em Python e compreender como ocorre o aprendizado do agente inteligente. E para isso vamos mandar nosso agente para a Gym.\n",
    "\n",
    "O OpenAI Gym é um ToolKit open-source que facilita o desenvolvimento e comparação de algoritmos de Aprendizagem Por Reforço. Viste o site para compreender mais sobre o Gym: https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote gym\n",
    "!pip install -q gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala a extensão do pco\n",
    "!pip install -q gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kVd1ovCggTd"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "env = gym.make(\"Boxing-ram-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObBpwav6gsLb"
   },
   "outputs": [],
   "source": [
    "# Função para criar o estado\n",
    "def cria_estado(atributos):\n",
    "    return int(\"\".join(map(lambda atributo: str(int(atributo)), atributos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmos de Machine Learning são representados como Classes. De fato, a Programação Orientada a Objetos é a base de programação em Machine Learning.\n",
    "\n",
    "Vamos criar uma Classe para o Algoritmo QLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MuXjDGxYgwi5"
   },
   "outputs": [],
   "source": [
    "# Classe para o Algoritmo QLearn\n",
    "class QLearn:\n",
    "    \n",
    "    # Método construtor\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        \n",
    "        # exploration constant\n",
    "        self.epsilon = epsilon  \n",
    "        \n",
    "        # discount constant\n",
    "        self.alpha = alpha  \n",
    "        \n",
    "        # discount factor\n",
    "        self.gamma = gamma  \n",
    "        \n",
    "        \n",
    "        self.actions = actions\n",
    "\n",
    "    # Método para obter o valor de Q\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    # Método para o aprendizado de Q\n",
    "    def aprendeQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Algoritmo Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
    "        '''\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def escolheAcao(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q);\n",
    "            mag = max(abs(minQ), abs(maxQ))\n",
    "            \n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "            # return random.choice(self.actions)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values\n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "        action = self.actions[i]\n",
    "        if return_q:  # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma * maxqnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRZLjCd1luH4"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_log(episode, g):\n",
    "    f = open(f'./log.csv', mode='a+')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([episode, g])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ac8PoSZXg8AX"
   },
   "outputs": [],
   "source": [
    "def learn_one_episode(Q, episode):\n",
    "  done = False\n",
    "  G, reward = 0, 0\n",
    "  state = env.reset()\n",
    "  while not done:\n",
    "      action = Q.chooseAction(build_state(state))\n",
    "      state2, reward, done, info = env.step(action)\n",
    "      Q.learn(build_state(state), action, reward, build_state(state))\n",
    "      G += reward\n",
    "      state = state2\n",
    "  csv_log(episode, G)\n",
    "  print('Episode {} Total Reward: {}'.format(episode, G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "94qsRoYyg2Ao",
    "outputId": "0878e85e-7801-47d6-d13d-fcfa57ea19c5"
   },
   "outputs": [],
   "source": [
    "Q = QLearn(list(range(0, 18)), 0.4, 0.618, 0.9)\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)\n",
    "\n",
    "Q.epsilon = 0.3\n",
    "Q.alpha = 0.518\n",
    "Q.gamma = 0.8\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)\n",
    "\n",
    "\n",
    "Q.epsilon = 0.2\n",
    "Q.alpha = 0.418\n",
    "Q.gamma = 0.7\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)\n",
    "\n",
    "Q.epsilon = 0.1\n",
    "Q.alpha = 0.318\n",
    "Q.gamma = 0.6\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "qlearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
